model:
  hidden_size: 128
  latent_size: 32
  num_layers: 2
  dropout: 0.2

training:
  batch_size: 32
  epochs: 8
  lr: 0.001
  seq_len: 10